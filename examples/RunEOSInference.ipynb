{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "This tutorial demonstrates how to sample EOS parameters from BNS data. The input is a set of MCMC runs for each BNS event that contain the parameters ($\\mathcal{M}$, $q$, $\\tilde\\Lambda$). The code makes the assumption that the MCMC runs were done using flat (uniform) priors in $q$ and $\\tilde\\Lambda$. If the priors were not flat to begin with, you must reweight the posterior in such a way that the reweighted samples correspond to a flat prior.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To run eosinference, you will have to install the following packages:\n",
    " * pandas: `pip install --user pandas`\n",
    " * h5py: `pip install --user h5py`\n",
    " * emcee (I used version 2.2.1): `pip install --user emcee`\n",
    " * corner: `pip install --user corner`\n",
    " * lalsuite: I'm sorry you have to install this.\n",
    "\n",
    "\n",
    "## Estimating EOS parameters and NS properties from multiple BNS events\n",
    "\n",
    "You will have to run 4 scripts to produce a final plots and output page.\n",
    "\n",
    "1. `generate_likelihood.py`: This evaluates the quantity $\\ln(p(q, \\tilde\\Lambda))$ on a grid using a bounded 2d KDE from the samples for $(q, \\tilde\\Lambda)$. This is the log(marginalized posterior). Because it is assumed that the prior in $(q, \\tilde\\Lambda)$ is flat, this just becomes a pseudo-likelihood for each BNS event. Because the uncertainty in chirp mass $\\mathcal{M}$ is so small, the chirp mass for each event is taken to be the mean value.\n",
    " * `--pefiles` The MCMC runs in CSV format. The column headers must contain ('mc', 'q', 'lambdat').\n",
    " * `--outfile` Name of the hdf5 file for the gridded likelihood function for each BNS. Also stores the mean chirpmass of each BNS.\n",
    " * `--qmin`, `--lambdatmax` The likelihood is approximated from the MCMC samples with a bounded_2d_kde. Its boundaries are \\[qmin, 1\\] and \\[0, lambdatmax\\]. You must make sure that no samples extend beyond these boundaries. Otherwise, the KDE method will not work correctly.\n",
    " * `--gridsize` The KDE approximation of the likelihood is gridded up. This is the number of grid points in each dimension (q, lambdat).\n",
    "\n",
    "\n",
    "2. `sample_distribution.py`: This script runs the sampler `emcee` on either the prior or the posterior for the N BNS events. \n",
    " * `--infile` The data file `pseudolikelihood.hdf5` that contains the chirp mass and gridded pseudolikelihoods for each BNS event.\n",
    " * `--outfile` Output of `emcee` in hdf5 format. Contains the chir masses, ln(posterior) and chains for each walker.\n",
    " * `--distribution` Either prior on posterior \n",
    " * `--nwalkers` Number of walkers for `emcee` must be atleast twice the number of sampled parameters >2*(N_BNS+N_EOS). The more the merrier. 64 works well.\n",
    " * `--niter`  \n",
    " * `--nthin` Thin the final output to reduce file size of outfile.\n",
    " * `--qmin` Minimum allowed mass ratio for the prior. This doesn't have to be the same as the value used to generate the 2d_bounded_kde in `generate_likelihood.py`.\n",
    " * `--mmin` Minimum allowed individual NS mass for the prior.\n",
    " * `--mmax` Maximum allowed individual NS mass for the prior.\n",
    " * `--maxmassmin` Minimum allowed value for the maximum NS mass (M_sun) calculated for the selected EOS parameters. This should be above the maximum known mass (1.93).\n",
    " * `--maxmassmax` Maximum allowed value for the maximum NS mass (M_sun) calculated for the selected EOS parameters. This should be less than the causality constraint (3.2).\n",
    " * `--csmax` Maximum allowed speed of sound (c=1 units).\n",
    " \n",
    " \n",
    "3. `calculate_ns_properties.py`: Once the mass ratios and EOS parameters are sampled for both the prior and posterior, this script calculates the following NS properties (for both the prior and posterior):\n",
    " * Maximum mass for each EOS sample.\n",
    " * Radius as a function of mass for each EOS sample.\n",
    " * $\\Lambda$ as a function of mass for each EOS sample.\n",
    " * Samples for (mass1, mass2, radius1, radius2, lambda1, lambda2) for each of the N BNS events. These are derived from the sampled values of $\\mathcal{M}$, $q$ and the EOS parameters. \n",
    " \n",
    " * `--priorfile` \n",
    " * `--posteriorfile` \n",
    " * `--outfile` \n",
    " * `--nburnin` Number of samples to remove from the prior and posterior files for burnin. \n",
    " * `--nthin` Thin the data in the files. If you have already thinned the samples in `sample_distribution.py`, this will reduce the samples by another factor.\n",
    " * `--nsample` The final number of samples to use for calculating NS properties. If you set it to a really high value, it will use the maximum number of available samples available after removing burnin and thinning.\n",
    "\n",
    "\n",
    "4. `generate_eos_output_page.py`: This script generates the final plots for the page `eos_output_page.html`.\n",
    " * `--infile` The ns_properties hdf5 file.\n",
    " * `--priorfile` hdf5 output file for prior emcee run.\n",
    " * `--posteriorfile` hdf5 output file for posterior emcee run.\n",
    " * `--outdir` Directory to output the final plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Reproducing the mass-radius results for GW170817\n",
    "\n",
    "Using samples for GW170817 with flat priors in $(q, \\tilde\\Lambda)$, These commands should reproduce the results for GW170817 found in [arXiv:1805.11581](https://arxiv.org/abs/1805.11581). The only difference is the choice of EOS parameterization.\n",
    "\n",
    "If you just want to see roughly what the output will look like, try these parameters (5-10 minutes):\n",
    "```\n",
    "NITER=100\n",
    "NBURNIN=20\n",
    "NTHIN=5\n",
    "NSAMPLE=1000\n",
    "```\n",
    "\n",
    "If you want accurate results, try these parameters (1-2 hours):\n",
    "```\n",
    "NITER=1000\n",
    "NBURNIN=200\n",
    "NTHIN=10\n",
    "NSAMPLE=5000\n",
    "```\n",
    "\n",
    "To really make sure you have properly converged chains and you are outside burnin, try 10,000 iterations (~1 day):\n",
    "```\n",
    "NITER=10000\n",
    "NBURNIN=5000\n",
    "NTHIN=10\n",
    "NSAMPLE=10000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# MCMC runs for each BNS system\n",
    "PEDATA0=\"RR30_reweight.csv\"\n",
    "\n",
    "# Output directory\n",
    "OUTDIR=\"gw170817_output\"\n",
    "\n",
    "# Sampling parameters\n",
    "NWALKERS=64\n",
    "\n",
    "# Fast run\n",
    "# NITER=100\n",
    "# NBURNIN=20\n",
    "# NTHIN=5\n",
    "# NSAMPLE=1000\n",
    "\n",
    "# Accurate run\n",
    "NITER=1000\n",
    "NBURNIN=200\n",
    "NTHIN=10\n",
    "NSAMPLE=5000\n",
    "\n",
    "# Very accurate run\n",
    "# NITER=10000\n",
    "# NBURNIN=5000\n",
    "# NTHIN=10\n",
    "# NSAMPLE=10000\n",
    "\n",
    "####### Start eosinference ########\n",
    "\n",
    "# Create an output directory\n",
    "mkdir $OUTDIR\n",
    "\n",
    "# Generate pseudolikelihood function ln(p)(q, lambdat) for each BNS event\n",
    "python ../bin/generate_likelihood.py \\\n",
    "--pefiles $PEDATA0 \\\n",
    "--outfile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--qmin 0.125 --lambdatmax 10000 --gridsize 250\n",
    "\n",
    "# Sample the prior\n",
    "python ../bin/sample_distribution.py \\\n",
    "--infile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--outfile ${OUTDIR}/prior.hdf5 \\\n",
    "--distribution prior \\\n",
    "--nwalkers $NWALKERS --niter $NITER --nthin 1 \\\n",
    "--qmin 0.125 --mmin 0.5 --mmax 3.2 \\\n",
    "--maxmassmin 1.93 --maxmassmax 3.2 --csmax 1.1\n",
    "\n",
    "# Sample the posterior\n",
    "python ../bin/sample_distribution.py \\\n",
    "--infile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--outfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--distribution posterior \\\n",
    "--nwalkers $NWALKERS --niter $NITER --nthin 1 \\\n",
    "--qmin 0.125 --mmin 0.5 --mmax 3.2 \\\n",
    "--maxmassmin 1.93 --maxmassmax 3.2 --csmax 1.1\n",
    "\n",
    "# Calculate NS properties for the downsampled parameters (burnin removed, thinned)\n",
    "python ../bin/calculate_ns_properties.py \\\n",
    "--priorfile ${OUTDIR}/prior.hdf5 \\\n",
    "--posteriorfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--outfile ${OUTDIR}/ns_properties.hdf5 \\\n",
    "--nburnin $NBURNIN --nthin $NTHIN --nsample $NSAMPLE\n",
    "\n",
    "# Generate output html page\n",
    "python ../bin/generate_eos_output_page.py \\\n",
    "--infile ${OUTDIR}/ns_properties.hdf5 \\\n",
    "--priorfile ${OUTDIR}/prior.hdf5 \\\n",
    "--posteriorfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--outdir ${OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Stacking parameter estimation results for 3 BNS events\n",
    "\n",
    "If you want to stack multiple BNS observations and measuer EOS parameters from the combined likelihood, just include a parameter estimation file for each BNS event. In this example, we just use three identical copies of the GW170817 parameter estimation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# MCMC runs for each BNS system\n",
    "PEDATA0=\"RR30_reweight.csv\"\n",
    "PEDATA1=\"RR30_reweight.csv\"\n",
    "PEDATA2=\"RR30_reweight.csv\"\n",
    "\n",
    "# Output directory\n",
    "OUTDIR=\"gw170817times3_output\"\n",
    "\n",
    "# Sampling parameters\n",
    "NWALKERS=64\n",
    "\n",
    "# Fast run\n",
    "# NITER=100\n",
    "# NBURNIN=20\n",
    "# NTHIN=5\n",
    "# NSAMPLE=1000\n",
    "\n",
    "# Accurate run\n",
    "NITER=1000\n",
    "NBURNIN=200\n",
    "NTHIN=10\n",
    "NSAMPLE=5000\n",
    "\n",
    "# Very accurate run\n",
    "# NITER=10000\n",
    "# NBURNIN=5000\n",
    "# NTHIN=10\n",
    "# NSAMPLE=10000\n",
    "\n",
    "####### Start eosinference ########\n",
    "\n",
    "# Create an output directory\n",
    "mkdir $OUTDIR\n",
    "\n",
    "# Generate pseudolikelihood function ln(p)(q, lambdat) for each BNS event\n",
    "python ../bin/generate_likelihood.py \\\n",
    "--pefiles $PEDATA0 $PEDATA1 $PEDATA2 \\\n",
    "--outfile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--qmin 0.125 --lambdatmax 10000 --gridsize 250\n",
    "\n",
    "# Sample the prior\n",
    "python ../bin/sample_distribution.py \\\n",
    "--infile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--outfile ${OUTDIR}/prior.hdf5 \\\n",
    "--distribution prior \\\n",
    "--nwalkers $NWALKERS --niter $NITER --nthin 1 \\\n",
    "--qmin 0.125 --mmin 0.5 --mmax 3.2 \\\n",
    "--maxmassmin 1.93 --maxmassmax 3.2 --csmax 1.1\n",
    "\n",
    "# Sample the posterior\n",
    "python ../bin/sample_distribution.py \\\n",
    "--infile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--outfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--distribution posterior \\\n",
    "--nwalkers $NWALKERS --niter $NITER --nthin 1 \\\n",
    "--qmin 0.125 --mmin 0.5 --mmax 3.2 \\\n",
    "--maxmassmin 1.93 --maxmassmax 3.2 --csmax 1.1\n",
    "\n",
    "# Calculate NS properties for the downsampled parameters (burnin removed, thinned)\n",
    "python ../bin/calculate_ns_properties.py \\\n",
    "--priorfile ${OUTDIR}/prior.hdf5 \\\n",
    "--posteriorfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--outfile ${OUTDIR}/ns_properties.hdf5 \\\n",
    "--nburnin $NBURNIN --nthin $NTHIN --nsample $NSAMPLE\n",
    "\n",
    "# Generate output html page\n",
    "python ../bin/generate_eos_output_page.py \\\n",
    "--infile ${OUTDIR}/ns_properties.hdf5 \\\n",
    "--priorfile ${OUTDIR}/prior.hdf5 \\\n",
    "--posteriorfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--outdir ${OUTDIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Using lower *AND* upper limit prior on the maximum NS mass\n",
    "\n",
    "Many people believe that GW170817 promptly collapsed to a black hole after merger. If this is the case, then we have a decent estimate that the maximum NS mass $M_{\\rm max}$ is below the gravitational (*not* baryonic) mass of the combined 2 NSs in GW170817. We can take this into account by reducing the allowed range for the NS maximum mass in the prior. For example, if we think the gravitational mass of the remnant black hole is $2.2M_\\odot$ and we also know that a $1.93M_\\odot$ star exists, then we can use the following bounds in the prior: `--maxmassmin 1.93 --maxmassmax 2.2`. You can also set the prior on each NS mass to `--mmax 2.2` as well, but this is not necessary.\n",
    "\n",
    "Note: If there was no mass ejection, the baryonic mass of the remnant is the sum of the baryonic masses of the components. From this, we can calculate the gravitational mass of the remnant BH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# MCMC runs for each BNS system\n",
    "PEDATA0=\"RR30_reweight.csv\"\n",
    "FINALBHMASS=2.2\n",
    "\n",
    "# Output directory\n",
    "OUTDIR=\"gw170817bhremnant_output\"\n",
    "\n",
    "# Sampling parameters\n",
    "NWALKERS=64\n",
    "\n",
    "# Fast run\n",
    "# NITER=100\n",
    "# NBURNIN=20\n",
    "# NTHIN=5\n",
    "# NSAMPLE=1000\n",
    "\n",
    "# Accurate run\n",
    "NITER=1000\n",
    "NBURNIN=200\n",
    "NTHIN=10\n",
    "NSAMPLE=5000\n",
    "\n",
    "# Very accurate run\n",
    "# NITER=10000\n",
    "# NBURNIN=5000\n",
    "# NTHIN=10\n",
    "# NSAMPLE=10000\n",
    "\n",
    "\n",
    "####### Start eosinference ########\n",
    "\n",
    "# Create an output directory\n",
    "mkdir $OUTDIR\n",
    "\n",
    "# Generate pseudolikelihood function ln(p)(q, lambdat) for each BNS event\n",
    "python ../bin/generate_likelihood.py \\\n",
    "--pefiles $PEDATA0 \\\n",
    "--outfile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--qmin 0.125 --lambdatmax 10000 --gridsize 250\n",
    "\n",
    "# Sample the prior\n",
    "python ../bin/sample_distribution.py \\\n",
    "--infile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--outfile ${OUTDIR}/prior.hdf5 \\\n",
    "--distribution prior \\\n",
    "--nwalkers $NWALKERS --niter $NITER --nthin 1 \\\n",
    "--qmin 0.125 --mmin 0.5 --mmax $FINALBHMASS \\\n",
    "--maxmassmin 1.93 --maxmassmax $FINALBHMASS --csmax 1.1\n",
    "\n",
    "# Sample the posterior\n",
    "python ../bin/sample_distribution.py \\\n",
    "--infile ${OUTDIR}/pseudolikelihood.hdf5 \\\n",
    "--outfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--distribution posterior \\\n",
    "--nwalkers $NWALKERS --niter $NITER --nthin 1 \\\n",
    "--qmin 0.125 --mmin 0.5 --mmax $FINALBHMASS \\\n",
    "--maxmassmin 1.93 --maxmassmax $FINALBHMASS --csmax 1.1\n",
    "\n",
    "# Calculate NS properties for the downsampled parameters (burnin removed, thinned)\n",
    "python ../bin/calculate_ns_properties.py \\\n",
    "--priorfile ${OUTDIR}/prior.hdf5 \\\n",
    "--posteriorfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--outfile ${OUTDIR}/ns_properties.hdf5 \\\n",
    "--nburnin $NBURNIN --nthin $NTHIN --nsample $NSAMPLE\n",
    "\n",
    "# Generate output html page\n",
    "python ../bin/generate_eos_output_page.py \\\n",
    "--infile ${OUTDIR}/ns_properties.hdf5 \\\n",
    "--priorfile ${OUTDIR}/prior.hdf5 \\\n",
    "--posteriorfile ${OUTDIR}/posterior.hdf5 \\\n",
    "--outdir ${OUTDIR}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
